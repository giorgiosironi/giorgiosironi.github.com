- Lee Sedol
-- Go has a much larger branching factor wrt Chess, much more difficult game in terms of complexity and required computation to explore all possible moves
-- how did we get here?
- we have computers!
-- ENIAC: this is how a computer looks like
- Dartmouth and pictures of the founders of AI
We propose that a 2 month, 10 man study of artificial intelligence be carried out during the summer of 1956 at Dartmouth College in Hanover, New Hampshire. The study is to proceed on the basis of the conjecture that every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it. An attempt will be made to find how to make machines use language, form abstractions and concepts, solve kinds of problems now reserved for humans, and improve themselves. We think that a significant advance can be made in one or more of these problems if a carefully selected group of scientists work on it together for a summer.
-- John McCarthy (LISP), Claude Shannon (information theory), Marvin Minsky
- perceptron
-- inputs are real numbers
-- they are linearly combined with constants called weights multiplied to each of them 
-- f is called activation function and it's the non-linearity (e.g. sigmoid)
- training
-- show the set for OR function, limited to 4 entries but imagine
-- derivative
- limited: can only learn linear boundary (verify) but not XOR
- XOR perceptron net
-- a network with 1 hidden layer can approximate any real function of real variables
-- back propagation
- In from three to eight years we will have a machine with the general intelligence of an average human being.
-- Marvin Minsky
-- 1970
- AI winters (80s, also 90s and first 2000s where SVMs were considered state of the art)
- deep learning
-- maaany layers
-- and much training
--- " This data set contains 29.4 million positions from 160,000 games played by KGS 6 to 9 dan human players"
--- "The value network was trained for 50 million mini-batches of 32 positions, using 50 GPUs, for one week."
- convolutional neural network
- hyperparameters and new problems
-- how many layers?
-- how many neurons in which layer?
-- which activation functions for which layer?
-- introduces new problems: vanishing gradients
- Keras example
